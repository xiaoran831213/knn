%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation LaTeX Template Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License: CC BY-NC-SA 3.0
% (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ----------------------------------------------------------------------------------------
% PACKAGES AND THEMES
% ----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

  % The Beamer class comes with a number of default slide themes which
  % change the colors and layouts of slides. Below this is a list of
  % all the themes, uncomment each in turn to see what they look like.

  % \usetheme{default} \usetheme{AnnArbor} \usetheme{Antibes}
  % \usetheme{Bergen} \usetheme{Berkeley} \usetheme{Berlin}
  % \usetheme{Boadilla} \usetheme{CambridgeUS} \usetheme{Copenhagen}
  % \usetheme{Darmstadt} \usetheme{Dresden} \usetheme{Frankfurt}
  % \usetheme{Goettingen} \usetheme{Hannover} \usetheme{Ilmenau}
  % \usetheme{JuanLesPins} \usetheme{Luebeck}
  \usetheme{Madrid}
  % \usetheme{Malmoe} \usetheme{Marburg} \usetheme{Montpellier}
  % \usetheme{PaloAlto} \usetheme{Pittsburgh} \usetheme{Rochester}
  % \usetheme{Singapore} \usetheme{Szeged} \usetheme{Warsaw}

  % As well as themes, the Beamer class has a number of color themes
  % for any slide theme. Uncomment each of these in turn to see how it
  % changes the colors of your current slide theme.

  % \usecolortheme{albatross}
  \usecolortheme{beaver}
  % \usecolortheme{beetle} \usecolortheme{crane}
  % \usecolortheme{dolphin} \usecolortheme{dove} \usecolortheme{fly}
  % \usecolortheme{lily} \usecolortheme{orchid} \usecolortheme{rose}
  % \usecolortheme{seagull} \usecolortheme{seahorse}
  % \usecolortheme{whale} \usecolortheme{wolverine}

  % \setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
  % \setbeamertemplate{footline}[page
  % number] % To replace the footer line in all slides with a simple slide count uncomment this line

  % \setbeamertemplate{navigation
  % symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\include{xtong}
\newcommand{\xmx}{\bs{X}} \newcommand{\xmt}{\bs{X}^{\prime}}
\newcommand{\imx}{\bs{I}} \newcommand{\umx}{\bs{U}}
\newcommand{\umt}{\bs{U}^{\prime}} \newcommand{\yvc}{\bs{y}}
\newcommand{\yht}{\hat{\bs{y}}}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

% ----------------------------------------------------------------------------------------
% TITLE PAGE
% ----------------------------------------------------------------------------------------

\title[Kernel Genomics]{Meta analysis of Variance Components \\
  simulation study}

\author{Xiaoran Tong} % Your name
\institute[EPI Biosta,
MSU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{ Michigan State University \\ % Your institution for the title page
  \medskip \textit{tongxia1@msu.edu} \\% Your email address
  \textit{qlu@epi.msu.edu} % Your email address
} \date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
  \titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
  \frametitle{Table of
    Content} % Table of contents slide, comment this block out to remove it
  \tableofcontents
\end{frame}
% ----------------------------------------------------------------------------------------
% PRESENTATION SLIDES
% ----------------------------------------------------------------------------------------
\section{Introduction}
% ----------------------------------------------------------------------------------------
\begin{frame}
  \frametitle{Introduction} %
  \textbf{Variance Component Model (VCM):} \\
  are suitable to depict the influence of ultra high dimentional $\xx$
  on $\vy$, with an typical construct of
  \begin{align}\label{eq:vcm}
    h(\vy) \sim \mathcal{N}(0, \xv), \quad
    \xv = \xv_{\bs{e}} + \xv_{\xx} = \sigma^2_0 \id + \sum_{i=1}^L
    \sigma^2_i \mathcal{K}_i(\xx)
  \end{align}
  \begin{itemize}
  \item $h$ transform $\vy$ into a sample following multivariate
    normrl (MVN) of mean $\bs{0}^N$ and covariance $\xv^{N \times N}$;
  \item $\xv$ is determined partially by $\xx$ through $L$ kernels:
    $\{\mathcal{K}_{1 \dots L}\}$, while the rest is left to an white
    noise $\id$;
  \item kernels are weighted by the to be fitted \textbf{variance
      component (VC)}
    $\vtheta = \{\sigma^2_0, \sigma^2_1 \dots \sigma^2_L\}$.
  \end{itemize}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}
  Despite being able to handle problems of ultra dimensionality, \\
  \textbf{VCM has its own issues}:
  \begin{itemize}
  \item large sample is required to stable a VCM;
  \item inevitable $O(N^3)$ matrix inversion;
  \item heterogeneous effect of $\xx$ when many variants are involved.
  \end{itemize}
  \textbf{Which motivate the meta-analysis based on VCM (MVCM)}:
  \begin{itemize}
  \item pooling large sample;
  \item distributed analytical load;
  \item model aggregation to counter heterogeneity;
  \end{itemize}
\end{frame}
% ----------------------------------------------------------------------------------------
\section{Meta-VCM}
\begin{frame}\frametitle{meta-VCM procedure}
  Let $\vc_i=(\vy_i, \xx_i)$ denote the i th. participating cohort; \\
  \textbf{{\color{red}meta}-analysis:}
  \begin{itemize}
  \item $\xc = \{\vc_1, \dots, \vc_Q \}$, $Q$ cohorts joined the consortium;
  \item $\hat{\vtheta}=\{\hat{\vtheta}_1, \dots, \hat{\vtheta}_Q\}$, develop Q models;
  \item $\hat{\vtheta} = \frac{\sum_i^Q \vw_i
      \hat{\vtheta}_i}{\sum_i^S \vw_i}$, weighted model aggregation
  \end{itemize}
  \textbf{{\color{red}mega}-analysis:}
  \begin{itemize}
  \item develop 1 model with the entire consortium;
  \item (included here as reference.)
  \end{itemize}
\end{frame}
% ----------------------------------------------------------------------------------------
\newcommand{\se}[1]{\hat{\mathtt{se}}\left(#1\right)}
\newcommand{\ti}{{\tilde{i}}}
\newcommand{\ef}{{\mathtt{o}}}
\begin{frame}\frametitle{weight scheme}
  Let $i = 1 \dots Q$ index the cohorts, $j = 1 \dots L$ index the VC(s).\\
  \textbf{weight by confidence:}
  \begin{align}
    w_{i,j} &= \frac{\left[\se{\theta_{i,j}} \right ]^2}
    {\sum_\ti \left[\se{\theta_{\ti,j}} \right ]^2} \\
    w_{i,j} &= \frac{n_i}{\sum_\ti n_\ti}
  \end{align}
  {\color{blue}\textbf{no extra analysis required}}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{weight scheme, continued}
  \textbf{weight by relative validity:}
  \begin{align}
    \gamma_i &= \frac{\sum_{r \in i^-}{\ef( \vc_i; \vtheta_r)}}
    {\ef(\vc_i; \vtheta_i)} \\
    w_{i,j}  &= \frac{\gamma_i}{\sum_\ti \gamma_\ti}
  \end{align}
  the function $\ef(\vc;\vtheta)$ tells in standard space how well
  model $\vtheta$ fits cohort $\vc$, e.g., the geometric mean
  likelihood
  \begin{align}
    o_\mathtt{GML}(\vc; \vtheta) = \sqrt[n]{\mathcal{N}(\bs{0}, \xv_{\vc, \vtheta})}
  \end{align}
  {\color{red}\textbf{dilemma: some party have to compute $o(\vc; \vtheta)$}}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{information completness}
  An meta-analysis does not cover all sample pairs in its kernel, an
  mega-analysis does.
  \begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{img/meta-mega}
    \caption{infomration completeness}
    \label{fig:info_comp}
  \end{figure}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{logistical concerns}
  \begin{table}[h]
    \label{tb:cost}
    \begin{tabular}{|l|l|l|l|}
      \hline
      \textbf{actions}       & \textbf{meta} & \textbf{mix}  & \textbf{mega} \\ \hline
      client prepare kernels & $O(n^2PL)$    & $O(n^2PL)$    & no            \\ \hline
      client fit VCM         & $O(n^3C)$     & no            & no            \\ \hline
      client validate VCM(s) & $O(n^3Q)$     & no            & no            \\ \hline
      server prepare kernels & no            & no            & $O(n^2Q^2PL)$ \\ \hline
      server fit VCM         & no            & $O(n^3CQ)$    & $O(n^3Q^3C)$  \\ \hline
      server validate VCM(s) & no            & $O(n^3Q^2)$   & no            \\ \hline
    \end{tabular}
    \caption{computation required}
  \end{table}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{logistical concerns, continue}
  \begin{table}[h]
    \label{tb:trust}
    \begin{tabular}{|l|c|c|c|}
      \hline
      \textbf{actions}        & \textbf{meta} & \textbf{mix}  & \textbf{maga} \\ \hline
      client prepare kernels  & no            & no            &               \\ \hline
      client fit VCM          & no            &               &               \\ \hline
      client validate VCM(s)  & no            &               &               \\ \hline
      server prepare kernels  &               &               & high          \\ \hline
      server fit VCM          &               & moderate      & moderate      \\ \hline
      server validate VCM(s)  &               & moderate      & moderate      \\ \hline
    \end{tabular}
    \caption{trust required}
  \end{table}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{logistical concerns, continue}
  \begin{table}[]
    \begin{tabular}{|l|c|c|c|}
      \hline
      \textbf{action}        & \textbf{meta} & \textbf{mix} & \textbf{maga} \\ \hline
      client prepare kernels & low           & low          &               \\ \hline
      client fit VCM         & low           &              &               \\ \hline
      client validate VCM    & high          &              &               \\ \hline
      server prepare kernels &               &              & no            \\ \hline
      server fit VCM         &               & no           & no            \\ \hline
      server validate VCM    &               & no           & no            \\ \hline
    \end{tabular}
    \caption{incentive required}
  \end{table}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}
  \begin{itemize}\frametitle{logistical concern, summarize}
  \item meta-analysis:
    \begin{itemize}
    \item least resource, almost no server involvement;
    \item rely on volunteerism to run validation test;
    \end{itemize}
  \item maga-analysis
    \begin{itemize}
    \item completness of information
    \item server should be $Q^2$ more powerful than an average client
    \item require enormous trust with raw data.
    \end{itemize}
  \item mixed-analysis
    \begin{itemize}
    \item validation analysis is readily applicable.
    \item have to trust the server with ``encrypted'' data.
    \end{itemize}
  \end{itemize}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{access model validity}
  A VCM
  $\hat{\vtheta}=\{\hat{\sigma}^2_0, \hat{\sigma}^2_1 \dots
  \hat{\sigma}^2_L\}$ so developed must strive to generalize better on
  new data $(\vy, \xx)$, gauged by the following criteria:
  \begin{block}{\textbf{MNL}: mean negative log likelihood}
    $\mathtt{MNL}(\vy, \xx; \hat{\vtheta}) = \frac{1}{n}
    [\frac{1}{2}\vy\xvh^{-1}\vy + \frac{1}{2}\log{|\xvh|} +
    \frac{n}{2}\log{2\pi}]$
  \end{block}
  \begin{block}{\textbf{MSE}: mean squre error}
    $\mathtt{MSE}(\vy, \xx; \hat{\vtheta}) = \frac{\sigma_0^4}{n}
    \vy^T\hat{\xv}^{-1} \hat{\xv}^{-1}\vy$
  \end{block}
  where,
  $\xvh = \xvh_e + \xvh_\vx = \hat{\sigma}^2_0\id +
  \sum_{i=1}^L\hat{\sigma}_i^2 \mathcal{K}_i(\xx)$ are covariance
  composed from the new data by the meta-VCM $\hat{\vtheta}$; $n$
  is the size of new data.
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{simulation study}
  Validity of an Meta-VCM built on $4$ cohorts tested on one holdout cohort;
  \begin{figure}
    \raggedright \scriptsize
    \begin{itemize}
    \item left to right: increase of inter-cohort heterogeneity;\\
    \item sub plot, left to right
      \begin{itemize} \scriptsize
      \item avg: average validity of all 4 VCMs;
      \item nlk and ssz: model weighted by geometric mean liklihood
        and sample size;
      \item whl: mega-analysis
      \end{itemize}
    \end{itemize}
    \centering
    \includegraphics[width=\linewidth]{km2_mnq_s01.png}
  \end{figure}
  \normalsize
  \textbf{Information completeness is not necessarily advantageous
    when client cohorts are not homogeneous.}
\end{frame}
% ----------------------------------------------------------------------------------------
\section{Meta-analysis at cohort}
\begin{frame}\frametitle{Client Analysis}
  \textbf{Improved VCM package} \\
  \textbf{use MINQUE for speed}
  \begin{itemize}
  \item directly solve $\vtheta$ instead of iterative search;
  \item expanded the kernels to improve model capacity;
  \end{itemize}
  {\color{green} \textbf{treat a cohort as a consortium}}
  \begin{itemize}
  \item divide and apply meta-analysis to the batches;
  \item view the classical analysis as maga-analysis.
  \end{itemize}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{batch-analysis: procedure}
  Let $\vb_{j}=(\vy_{j}, \xx_{j})$ denote the $j$ th. batch of a cohort $\vc$; \\
  \textbf{batch-analysis:}
  \begin{itemize}
  \item $\vc = [\vb_1^{(k)T}, \dots, \vb_R^{(k)T} ]^T$, $R$ batches of
    cohort $\vc$;
  \item $\hat{\vtheta}=\{\hat{\vtheta}_1, \dots, \hat{\vtheta}_R\}$,
    get R estimates;
  \item Repeat for $S$ times;
  \item
    $\vthetah = \frac{\sum_j^R\sum_k^S w_j^{(k)} \vthetah_j^{(k)}}
    {\sum_{\tilde{j}}^R \sum_{\tilde{k}}^S
      w_{\tilde{j}}^{(\tilde{k})}}$, aggregate
  \end{itemize}
  \textbf{standard analysis:}
  \begin{itemize}
  \item get 1 estimate $\hat{\vtheta}$ for the entire cohort;
  \end{itemize}
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame}\frametitle{batch-analysis: as domestic meta-analysis}
  \textbf{meta-analysis in common sense $\approx$ batch-analysis of a
    consortium}
  \begin{align*}
    \vb_{ij} & \qquad \in & \vc_i  & \qquad \in & \xc \\
    batch    & \qquad \to & cohort & \qquad \to & consortium
  \end{align*}
  \textbf{batch-analysis in common sense $\approx$ meta-analysis of a cohort} \\
  batches are {\color{red}\textbf{random}} partition of a cohort,
  cohorts are {\color{red}\textbf{fixed}} partition of a consortium. \\
  despite the difference, batch-analysis of a cohort should inherit
  properties of mata-analysis applied to a consortium;
\end{frame}
% ----------------------------------------------------------------------------------------
\begin{frame} \frametitle{batch-analysis: simulation study} %
  \textbf{Validity of VCM trained by batch, tested on one holdout cohort:}
  \begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{img/bat_vcm_mse}
    \caption{batched versus standard analysis}
    \label{fig:bat}
  \end{figure}
\end{frame}
% ---------------------------------------------------------
\begin{frame} \frametitle{batch-analysis: simulation study}
  \textbf{Randomly pooled batches is robust against heterogeneity.}
\end{frame}
% ---------------------------------------------------------
\section{Theoratical Ground}
\begin{frame} \frametitle{Theoratical Ground:}%
  \textbf{why aggregated min-VCM outperform complete VCM?} \\
  Let $\ve=[e_1 \dots e_Q]$ be the error made by the $Q$ client models
  on any random future data point, such that
  \begin{itemize}
  \item $\EX(e_i) = v$
  \item $\EX(e_i e_j) = c$
  \end{itemize}
  The average error made by of all $Q$ models is
  $\frac{1}{k}\sum_i e_i$, and
  \begin{align}
    \EX\left[ \left(\frac{1}{k}\sum_i e_i\right)^2 \right]
    &= \frac{1}{k^2}\EX\left[\sum_i \left(e_i^2 + \sum_{j \ne i}e_ie_j \right)  \right] \\
    &= \frac{1}{k}v + \frac{k-1}{k}c
  \end{align}
\end{frame}
% ---------------------------------------------------------
\begin{frame}
  \frametitle{Simulation Settings:} %
  \textbf{Response Generation: heterogeneous effect}:\\
  \begin{itemize}
  \item the variance components varies across the $Q$ cohorts.
  \item the white noise and type of kernel is unchanged
    \begin{align*}
      \vz_j & \sim \mathcal{N}(\bs{0}, \sigma^2_0 \id + \sum_{i=1}^L
              \sigma_{i,j}^2 \mathcal{K}_i(\xx_j)), \quad
              \sigma_{i,j}^2 \sim \chi_2^2, \quad
              j = 1 \dots Q
    \end{align*}
  \item the total effect $\vz$ is a mix between the homogeneous and
    the cohort specifit heterogeneous effect.
  \end{itemize}
  \begin{align*}
    \vz & = \vz_h (1-\alpha) + [\vz_1^T, \dots, \vz_Q^T]^T \alpha,
          \quad \alpha \in [0, 1]
  \end{align*}
  Gradually increase the ratio of heterogeneity $alpha$ from 0 (fully
  homogeneous) to 1 (fully heterogeneous).
\end{frame}
\begin{frame}
  Generating heterogeneous effect:
  \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{kma_sim_1kg.png}
  \end{figure}
\end{frame}
% ---------------------------------------------------------
\begin{frame}
  \frametitle{Simulation Settings:} %
  \textbf{Response Generation: Transformation on $\vy$:}
  \begin{itemize}
  \item the inverse transformation $g=h^{-1}$ is applied:
    $\vy=g(\vz)$.
  \item $g$ can be a distribution morph:
    \begin{itemize}
    \item normal $\bs{z}$ morph to student $t_{10}$ for outliers;
    \item normal $\bs{z}$ morph to $\mathcal{B}(p)$ for discretion;
    \end{itemize}
    derive the probability $p_*$ of $z_*$ either empirically, or from
    the diagnal of combined covariance; \\
    search $p_*$ in the quantile table of the target distribution for
    morphed value $y_*$;
  \item $g$ can also be a direct transform, for example:
    $$ y_* = \mathtt{sigmoid}(z_*), \quad \mathtt{or} \quad y_* = \mathtt{sin}(2\pi z_*) $$
  \end{itemize}
\end{frame}
% ---------------------------------------------------------% ---------------------------------------------------------
\begin{frame}
  \frametitle{Simulation Settings:} %
  \textbf{Training:}
  \begin{itemize}
  \item working model: normalized polynomial up to the 2nd order built
    from all variants $\xg$
    \begin{align}\label{eq:dvp}
      \xvh = \xvh_e + \xvh_\xg = \hat{\sigma}^2_0\id +
      \hat{\sigma}_1^2 (\frac{1}{P}{\xg \xg^T}) + \hat{\sigma}_2^2
      (\frac{1}{P}{\xg \xg^T})^2
    \end{align}
  \item for each cohort, use non-batched MINQUE for simplicity;
  \item pooling strategies:
    \begin{itemize}
    \item meta-analysis: aggregation of $Q$ sub-models
    \item mega-analysis: one model derived from the entire
      $N \times Q$ data
    \item average analyse: apply each sub-model to the testing data,
      get average performance, this is actually a non-pooling, but
      included for benchmark purpose.
    \end{itemize}
  \end{itemize}
\end{frame}
% ---------------------------------------------------------
\begin{frame}
  \frametitle{Simulation Settings:} %
  \textbf{Testing Performance} \\
  Let $(\vy, \xg)$ denote the testing cohort. First make a prediction
  of $\vyh$ assuming it is normal, even for MINQUE:
  \begin{align}
    \begin{split}
      \vyh &= \xvh_\xg \xvh^{-1} \vy,
    \end{split}
  \end{align}
  where $\xvh_\xg$ and $\xvh$ take the same form of (\ref{eq:dvp}).\\
  \textbf{Use the following performance criteria}
  \begin{itemize}
  \item
    $\mathtt{MSE} = \frac{1}{N} \norm{\vy, \hat{\vy}}_2^2 =
    \frac{1}{N} (\vy-\vyh)^T(\vy-\vyh)$
  \item
    $\mathtt{MNL} = \frac{1}{N} [\frac{1}{2}\vy\xvh^{-1}\vy +
    \frac{1}{2}\log{|\xvh|} + \frac{N}{2}\log{2\pi}]$
  \end{itemize}
\end{frame}
% ---------------------------------------------------------
\section{Simulation Results}
% ---------------------------------------------------------
\begin{frame}{EXP 01: Use MINQUE, NLK plotted}
  \scriptsize super plot, left to right: gradual increase of heterogeneity;\\
  top: response is normal; bottom: response morphed into $t_{10}$ \\
  \normalsize
  \begin{figure}
    \includegraphics[width=\linewidth]{km2_mnq_s01.png} \\
    \includegraphics[width=\linewidth]{km2_mnq_s02.png}
  \end{figure}
  \tiny sub plot, left to right, avg: average performance without pooling;\\
  nlk and ssz: meta-analysis, weighted by mean geometric liklihood, or sample size; \\
  whl: mega-analysis, model trained with the whole sample;
\end{frame}
% ---------------------------------------------------------
\section{Speculation}
\begin{frame}
  \frametitle{Speculation}
  \begin{itemize}
  \item increased inter-cohort heterogeneity cost the generalization
    of models built from mega-analyis.
  \item meta-analysis is robust to such heterogeneity.
  \item mana-analysis only suits for fully homogeneous populations.
  \item even without pooling of models (meta) or data (mega), the
    averge performance of sub-models are almost stable across levels
    of heterogeneity, showing the effect of a bagging assemble.
  \end{itemize}
\end{frame}
% ---------------------------------------------------------
\end{document}
