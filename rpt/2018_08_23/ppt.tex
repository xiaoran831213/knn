%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation LaTeX Template Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License: CC BY-NC-SA 3.0
% (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ----------------------------------------------------------------------------------------
% PACKAGES AND THEMES
% ----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

  % The Beamer class comes with a number of default slide themes which
  % change the colors and layouts of slides. Below this is a list of
  % all the themes, uncomment each in turn to see what they look like.

  % \usetheme{default} \usetheme{AnnArbor} \usetheme{Antibes}
  % \usetheme{Bergen} \usetheme{Berkeley} \usetheme{Berlin}
  % \usetheme{Boadilla} \usetheme{CambridgeUS} \usetheme{Copenhagen}
  % \usetheme{Darmstadt} \usetheme{Dresden} \usetheme{Frankfurt}
  % \usetheme{Goettingen} \usetheme{Hannover} \usetheme{Ilmenau}
  % \usetheme{JuanLesPins} \usetheme{Luebeck}
  \usetheme{Madrid}
  % \usetheme{Malmoe} \usetheme{Marburg} \usetheme{Montpellier}
  % \usetheme{PaloAlto} \usetheme{Pittsburgh} \usetheme{Rochester}
  % \usetheme{Singapore} \usetheme{Szeged} \usetheme{Warsaw}

  % As well as themes, the Beamer class has a number of color themes
  % for any slide theme. Uncomment each of these in turn to see how it
  % changes the colors of your current slide theme.

  % \usecolortheme{albatross}
  \usecolortheme{beaver}
  % \usecolortheme{beetle} \usecolortheme{crane}
  % \usecolortheme{dolphin} \usecolortheme{dove} \usecolortheme{fly}
  % \usecolortheme{lily} \usecolortheme{orchid} \usecolortheme{rose}
  % \usecolortheme{seagull} \usecolortheme{seahorse}
  % \usecolortheme{whale} \usecolortheme{wolverine}

  % \setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
  % \setbeamertemplate{footline}[page
  % number] % To replace the footer line in all slides with a simple slide count uncomment this line

  % \setbeamertemplate{navigation
  % symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\include{xtong}
\include{math_commands}
\newcommand{\se}[1]{\hat{\mathtt{se}}\left(#1\right)} % standard error
\newcommand{\ti}{{\tilde{i}}} % tilde i
\newcommand{\ef}{{\mathtt{o}}} % error function
\newcommand{\kn}{\mathcal{K}} % kernel
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{bm}

% ----------------------------------------------------------------------------------------
% TITLE PAGE
% ----------------------------------------------------------------------------------------

\title[Meta-VCM]{Meta analysis by Variance Component Model}

\author{Xiaoran Tong} % Your name
\institute[EPI Biosta,
MSU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{ Michigan State University \\ % Your institution for the title page
  \medskip \textit{tongxia1@msu.edu} \\% Your email address
  \textit{qlu@epi.msu.edu} % Your email address
} \date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
  \titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
  \frametitle{Table of
    Content} % Table of contents slide, comment this block out to remove it
  \tableofcontents
\end{frame}
% ----------------------------------------------------------------------------------------
% PRESENTATION SLIDES
% -------------------------------------------------------------------
\section{Motivation}
\begin{frame}\frametitle{Motivation: Issues}
  Analyzing ultra-high dimensional profiles are increasingly popular,
  e.g.,
  \begin{itemize}
  \item deeply sequenced genome
  \item neural imaginings (i.e., MRI, fMRI, DTI)
  \end{itemize}
  \textbf{Variance Component Model (VCM)} is a sensible choice to
  \begin{itemize}
  \item aggregate weak effect of massive number of correlated variants
  \item partially solve the curse of dimension: $N \times P \to N^2$
  \end{itemize}
  \textbf{VCM} is also known as
  \begin{itemize}
  \item kernel machine, kernel method
  \item random effect model
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\newcommand{\fit}[1]{{\color{magenta}{#1}}}
\newcommand{\CB}[1]{{\color{blue}{#1}}}
\newcommand{\CR}[1]{{\color{red}{#1}}}
\newcommand{\green}[1]{{\color{green}{#1}}}
\begin{frame}
  \frametitle{Motivation: \CB{v}ariance \CB{c}omponent
    \CB{m}odel (\CB{\textbf{VCM}})} %
  \textbf{VCM} depicts the influence of ultra high dimensional $\xx$
  on $\vy$,
  \begin{align}\label{eq:vcm}
    h(\vy) = \vz \sim \mathcal{N}(0, \xv), \quad
    \xv = \fit{\sigma^2_0} \id + \fit{\sigma^2_1} \mck_1(\xx) + \dots + \fit{\sigma^2_L} \mck_L(\xx)
  \end{align}
  \begin{itemize}
  \item as the residual of $\vy$, $\vz$ follows MVN of covariance
    $\xv$;
  \item $\xv$ is the sum of $L$ kernels plus a white noise
    $\mck_0 = \id$;
  \item kernels are built from $\xx$
  \item The to be fitted
    $\vtheta = \{\fit{\sigma^2_0}, \fit{\sigma^2_1} \dots
    \fit{\sigma^2_L}\}$ comprises a \textbf{VCM}.
  \end{itemize}

\end{frame}
% -------------------------------------------------------------------
\begin{frame}
  \frametitle{Motivation: computation issues of \CB{\textbf{VCM}}} %
  \textbf{A VCM requires}
  \begin{itemize}
  \item multiple kernels ($L>1$) to achieve high capacity;
  \item large sample $N$ to stabilize;
  \item $O(N^3)$ to invert the combined kernel matrix.
  \end{itemize}
  \textbf{Re-introduce the curse of dimensionality by large cohorts.}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}\frametitle{Motivation: \textbf{consider
      \CB{meta}-VCM:}} %
  \begin{figure}\includegraphics[width=.90\textwidth]{img/meta0}\end{figure}
  \textbf{\CR{decentralized!}}  \textbf{limited choice in model and
    algorithm;}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}\frametitle{Motivation: \textbf{consider
      \CB{mega}-VCM:}} %
  \begin{figure}\includegraphics[width=.90\textwidth]{img/mega0}\end{figure}
  \textbf{\CR{full information!} insecured data, renewed curse of
    dimension.}
\end{frame}
% -------------------------------------------------------------------
\section{Improved VCM}
% -------------------------------------------------------------------
\begin{frame}
  \large{\textbf{Proposal}}: \\
  \large{\textbf{\CR{Batched, Deep, MINQUE} \CB{v}ariance \CB{c}omponent \CB{m}odel}} \\
  \textbf{Increase capacity}
  \begin{itemize}
  \item depen the VCM via kernel enrichment;
  \end{itemize}
  \textbf{Reduce computation}
  \begin{itemize}
  \item solve VCs in close form by MINQUE;
  \item built VCM by batch.
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}%
  \frametitle{Kernel Enrichment}
  \textbf{``polynomial expansion'' by kernel:}
  \begin{align*}
    \xk^{(1)} & = \bigcup_{i}^L \kn_i(\xx) = \{\xk_1, \dots, \xk_L\} \\
    \xk^{(2)} & = \bigcup_{i,j}^{L \times L} \kn_i(\xx) \circ \kn_j(\xx) = \{\xk_1\xk_1, \xk_1\xk_2, \dots, \xk_L\xk_L\} \\
    \xk^{(3)} & = \bigcup_{i,j,k}^{L^{(3)}}  \kn_i(\xx) \circ \kn_j(\xx) \circ \kn_k(\xx)
  \end{align*}
  \CB{\textbf{improve model capacity.}}
\end{frame}
% -------------------------------------------------------------------
\begin{frame} %
  \frametitle{MINQUE} %
  \textbf{use \CB{MINQUE} for estimation}
  \begin{itemize}
  \item faster by solving $\vtheta=\{\sigma_0^2, \sigma_1^2, \dots \}$
    in close form;
  \item numerically more stable via generalized inverse;
  \item no assumption on $\vz$'s distribution.
  \end{itemize}
  MINQUE -- minimum norm quadratic unbiased estimation, was developed
  by (\textbf{Rao et. al.}, 1971).
\end{frame}
% -------------------------------------------------------------------
\begin{frame}\frametitle{Batched Training}
  \begin{enumerate}
  \item randomly partition the cohort into Q batches $b_i = (\vy_i, \mM_i, \mX_i)$;
  \item for $i = 1 \dots Q$:
    \begin{itemize}
    \item calculate $(\vr_i, \tK_i) = (h(\vy_i), \mathcal{K}_{1 \dots L}(\mX_i))$
    \item solve the $i$ th. VCM, $\vtheta_i^{(j)}$
    \end{itemize}
  \item repeat \textbf{1} and \textbf{2} for $j=1 \dots R$ epochs
  \item let $\hat{\vtheta} = \frac
    {\sum_{i=1}^Q\sum_{j=1}^R w_i^{(j)}\vtheta_i^{(j)}}
    {\sum_{\tilde{i}=1}^Q \sum_{\tilde{j}=1}^R w_{\tilde{i}}^{(\tilde{j})}}$
  \end{enumerate}
  When the sample size $N$ is huge, and the batch size $\frac{N}{Q}$ large enough to
  stabilize each VCM, a single run-through ($R=1$) is sufficient.
\end{frame}
% -------------------------------------------------------------------
\section{Simulation}
\begin{frame}
  \frametitle{Simulation: MINQUE versus REML} %
  \centering
  \Large{\textbf{Simulation Studies}}
  \normalsize
  \begin{itemize}
  \item whole sample model, MINQUE deep VCM versus GCTA-REML;
  \item batched training.
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}\frametitle{simulation: meta-analysis}
  \textbf{A deep VCM solved by MINQUE from 1K samples, tested on another 1K samples;} \\
  {\color{blue}\textbf{outer plots: the benchmarks, from left to right:}}
  \begin{itemize}
  \item \textbf{MSE:} mean square error;
  \item \textbf{NLK:} mean negative log likelihood;
  \item \textbf{RTM:} running time;
  \end{itemize}
  \CB{\textbf{the data generating models, from top to bottom:}}
  \begin{itemize}
  \item \textbf{order 1:} $h(\vy)=\va, \qquad \va \sim \mathcal{N}(\bm{0}, \sigma_0^2\id + \sigma_1\mX\mX^T)$;
  \item \textbf{order 2:} $h(\vy)=\va^2+\va$
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame} \frametitle{simulation: meta-analysis}
  \textbf{the cohorts are \color{blue}{homogeneous}:} \\
  % \begin{figure}
  %   \centering \includegraphics[width=.95\linewidth]{img/met_hom_stt_mnq_ssz}
  % \end{figure}
  \textbf{\color{blue}{inner plot: strategies, from left to right:}}
  \begin{itemize}
  \item \textbf{avg:} the test error of 8 VCMs, averaged.
  \item \textbf{mega/meta:} mega/meta-VCM
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}%
  \frametitle{simulation: meta-analysis} %
  \textbf{the cohorts are \color{red}{heterogeneous}:} \\
  % \begin{figure}
  %   \centering \includegraphics[width=.95\linewidth]{img/met_het_stt_mnq_ssz}
  % \end{figure}
  \textbf{\color{blue}{inner plot: strategies, from left to right:}}
  \begin{itemize}
  \item \textbf{avg:} the test error of 8 VCMs, averaged.
  \item \textbf{mega/meta:} mega/meta-VCM
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}%
  \frametitle{simulation: weight-by validity vs. precision, MSE} %
  \begin{figure}
    \centering \includegraphics[width=1.0\linewidth]{img/met_mnq_cyh_mse}
  \end{figure}
  \textbf{\color{blue}{inner plot: strategies, from left to right:}}
  \begin{itemize}
  \item \textbf{avg:} average testing error of all VCMs;
  \item \textbf{cyh:} meta-analysis weight by validity;
  \item \textbf{ssz:} meta-analysis weight by precision;
  \item \textbf{whl:} mega-analysis.
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}%
  \frametitle{simulation: weight-by validity vs. precision, NLK} %
  \begin{figure}
    \centering \includegraphics[width=1.0\linewidth]{img/met_mnq_cyh_nlk}
  \end{figure}
  \textbf{\color{blue}{inner plot: strategies, from left to right:}}
  \begin{itemize}
  \item \textbf{avg:} average testing error of all VCMs;
  \item \textbf{cyh:} meta-analysis weight by validity;
  \item \textbf{ssz:} meta-analysis weight by precision;
  \item \textbf{whl:} mega-analysis.
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame}\frametitle{simulation: per cohort analysis}
  \textbf{a client VCM tested on a new cohort;} \\
  \textbf{residual z follows $t_{10}$ instead of normal;} \\
  {\color{blue}\textbf{outer plots: the benchmark, from left to right:}}
  \begin{itemize}
  \item \textbf{MSE:} mean square error;
  \item \textbf{NLK:} mean negative log likelihood;
  \item \textbf{RTM:} running time;
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame} \frametitle{simulation: per cohort analysis}
  \begin{figure}
    \centering \includegraphics[width=.95\linewidth]{img/vcm_ply_mnq}
  \end{figure}
  {\color{blue}\textbf{inner plots, model and algorithms:}}
  \begin{itemize}
  \item \textbf{GCTA:} GCTA's REML, developed by (\textbf{Yang et. al.});
  \item \textbf{MNQ1:} MINQUE ;
  \item \textbf{MNQ2:} MINQUE with kernel enrichment
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
% \section{Theoratical Ground}
\begin{frame} <presentation:0> %
  \frametitle{Theoretical Ground} %
  \textbf{How meta-VCM built on incomplete data outperform maga-VCM?} \\
  Let $\ve=[e_1 \dots e_Q]$ be the validation error made by the $Q$
  client models on a unseen random data point, such that
  \begin{itemize}
  \item $\E(e_i) = v$
  \item $\E(e_i e_j) = c$
  \end{itemize}
  The average error made by all $Q$ models is $\frac{1}{k}\sum_i e_i$,
  and
  \begin{align} \label{eq:beg} \E\left[ \left(\frac{1}{k}\sum_i
        e_i\right)^2 \right]
    &= \frac{1}{k^2}\E\left[\sum_i \left(e_i^2 + \sum_{j \ne i}e_ie_j \right)  \right] \\
    &= \frac{1}{k}v + \frac{k-1}{k}c
  \end{align}
\end{frame}
% -------------------------------------------------------------------
\begin{frame} <presentation:0>%
  \frametitle{Theoretical Ground}%
  \textbf{when inter-cohort heterogeneity increases:}
  \begin{itemize}
  \item VCMs become less correlated, $\E(e_i e_j) = c$ goes to 0;
  \item $\E\left[ \left(\frac{1}{k}\sum_i e_i\right)^2 \right]$
    shrinks to $\frac{v}{k}$ -- averaging VCMs benefits;
  \item heterogeneity act as noise for the full, mega-VCM;
  \end{itemize}
  \textbf{when cohort become more homogeneous:}
  \begin{itemize}
  \item VCMs become more correlated, $\E(e_i e_j) = c$ goes to 1;
  \item $\E\left[ \left(\frac{1}{k}\sum_i e_i\right)^2 \right]$
    remains to be $v$ -- averaging VCMs has no effect;
  \item the whole data is less noisy to the full, mega-VCM;
  \end{itemize}
  The model averaging (\ref{eq:beg}) is called ``bootstrap aggregating
  (bagging)'', developed by (Breiman, 1994, et. al.).
\end{frame}
% -------------------------------------------------------------------
\section{Summary}
\begin{frame} %
  \frametitle{Summary} %
  \textbf{Kernel-Meta-VCM} manages to combine the strength of
  meta-analysis, kernel, and variance component model.
  \begin{itemize}
  \item \textbf{``Meta''}
    \begin{itemize}
    \item inherits the economical advantage of distributed research;
    \end{itemize}
  \item \textbf{``VCM''}
    \begin{itemize}
    \item overcome the curse of dimensionality;
    \end{itemize}
  \item \textbf{``Meta'' + ``VCM''}
    \begin{itemize}
    \item robust against inter-cohort heterogeneity.
    \end{itemize}
  \item \textbf{``Kernel'' and ``residual'' uploads}
    \begin{itemize}
    \item balance flexibility, security, and work load;
    \end{itemize}
  \end{itemize}
\end{frame}
% -------------------------------------------------------------------
\begin{frame} %
  \frametitle{Contact} %
  Xiaoran Tong \\
  Department of Epidemiology \& Biostatistics \\
  Michigan State University
  \begin{itemize}
  \item email: tongxia1@msu.edu
  \item phone: (1)517-220-1229
  \end{itemize}
\end{frame}
\end{document}
